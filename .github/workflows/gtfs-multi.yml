name: Build & Deploy GTFS Mapper

on:
  push:
    branches: [ main ]
    paths:
      - ".github/workflows/gtfs-multi.yml"
      - "gtfs-mapper/automation/feeds.json"
      - "gtfs-mapper/automation/overrides.json"
      - "gtfs-mapper/automation/overrides-*.json"   # per-slug overrides (e.g., overrides-flixbus.json)
      - "gtfs-mapper/automation/rebuild-*.js"       # any builder variants you add
  workflow_dispatch: {}
  schedule:
    - cron: "0 0 * * 0"    # weekly on Sunday at midnight

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: gtfs-mapper

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: gtfs-mapper/package-lock.json

      - name: Install
        run: npm ci --no-audit --no-fund

      - name: Configure Pages
        uses: actions/configure-pages@v5

      # OPTIONAL: expose stable JS/CSS if dist/ exists
      - name: Make app assets available to site
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p site
          if [ -d dist ]; then
            JS="$(find dist -type f -name '*.js' | head -n1 || true)"
            CSS="$(find dist -type f -name '*.css' | head -n1 || true)"
            [ -n "${JS:-}" ] && cp "$JS" site/app.js || true
            [ -n "${CSS:-}" ] && cp "$CSS" site/style.css || true
          else
            echo "No dist/ directory; skipping app.js/style.css"
          fi

      - name: Build site per feed (download, compile with overrides, index)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p site

          # Build TSV: slug \t url (supports [{slug,url}], {feeds:[...]}, ["slug"])
          if [ ! -f automation/feeds.json ]; then
            echo "ERROR: gtfs-mapper/automation/feeds.json not found"; exit 1
          fi
          node -e '
            const fs = require("fs");
            const raw = JSON.parse(fs.readFileSync("automation/feeds.json","utf8"));
            const list = Array.isArray(raw) ? raw : (raw.feeds || []);

            const norm = list.map(x => {
              if (typeof x === "string") {
                return { slug: x, url: "", localPath: "", apiKeyHeader: "", apiKey: "" };
              }
              if (!x || typeof x !== "object") return null;
              return {
                slug: x.slug,
                url: x.url || "",
                localPath: x.localPath || "",
                apiKeyHeader: x.apiKeyHeader || "",
                apiKey: x.apiKey || ""
              };
            }).filter(x => x && x.slug);

            for (const it of norm) {
              const cols = [it.slug, it.url, it.localPath, it.apiKeyHeader, it.apiKey]
                .map(v => String(v ?? "").replace(/\t/g, " "));
              console.log(cols.join("\t"));
            }
          ' > feeds.tsv
          # Save slugs for registry/admin
          cut -f1 feeds.tsv | grep -v "^[[:space:]]*$" > feeds.txt

          # Helper to fetch source GTFS (kept for debugging headers)
          fetch_with_diag() {
            local url="$1" out="$2" api_header="$3" api_key="$4" dir
            dir="$(dirname "$out")"
            mkdir -p "$dir"
            local hdr="$dir/headers.txt"
            local raw="$dir/download.raw"
            rm -f "$hdr" "$raw" "$out" "$out.part" "$dir/error.html"

            echo "---- HEAD $url"
            if [ -n "${api_header}" ] && [ -n "${api_key}" ]; then
              curl -sS -I -L -A "gtfs-mapper-ci" -H "${api_header}: ${api_key}" "$url" || true
            else
              curl -sS -I -L -A "gtfs-mapper-ci" "$url" || true
            fi

            echo "---- GET  $url"
            if [ -n "${api_header}" ] && [ -n "${api_key}" ]; then
              if ! curl -L -sS \
                  -H "Accept: application/zip, application/octet-stream,*/*" \
                  -H "${api_header}: ${api_key}" \
                  -A "gtfs-mapper-ci" \
                  -D "$hdr" \
                  --connect-timeout 30 \
                  --max-time 900 \
                  --retry 5 --retry-delay 5 --retry-connrefused \
                  -C - \
                  -o "$raw" \
                  "$url"; then
                echo "Curl failed (network/HTTP). See $hdr and $raw"
                return 1
              fi
            else
              if ! curl -L -sS \
                  -H "Accept: application/zip, application/octet-stream,*/*" \
                  -A "gtfs-mapper-ci" \
                  -D "$hdr" \
                  --connect-timeout 30 \
                  --max-time 900 \
                  --retry 5 --retry-delay 5 --retry-connrefused \
                  -C - \
                  -o "$raw" \
                  "$url"; then
                echo "Curl failed (network/HTTP). See $hdr and $raw"
                return 1
              fi
            fi

            # ZIP magic (PK\x03\x04) + deep validation (unchanged)
            if head -c 4 "$raw" | hexdump -Cv | grep -q "50 4b 03 04"; then
              mv "$raw" "$out"
              if ! unzip -tqq "$out" >/dev/null 2>&1; then
                echo "Downloaded file is not a valid ZIP (unzip test failed)."
                return 2
              fi
              return 0
            else
              echo "Response is not ZIP; leaving as error.html for debugging."
              mv "$raw" "$dir/error.html"
              return 3
            fi
          }

          any_fail=0
          while IFS=$'\t' read -r SLUG URL LOCAL_PATH API_HEADER API_KEY; do
            [ -z "${SLUG:-}" ] && continue
            mkdir -p "site/${SLUG}"

            # Optional: keep headers/download debug page
            URL_DISPLAY="${URL:-}"
            LOCAL_NOTE=""
            API_HEADER="${API_HEADER:-}"
            API_KEY="${API_KEY:-}"

            if [ -n "${URL:-}" ]; then
              echo "::group::Downloading ${SLUG}"
              echo "URL: ${URL}"
              if [ -n "${API_HEADER}" ] && [ -n "${API_KEY}" ]; then
                echo "Using API header: ${API_HEADER}"
              fi

              if fetch_with_diag "${URL}" "site/${SLUG}/gtfs.zip" "${API_HEADER}" "${API_KEY}"; then
                LOCAL_NOTE='<a href="gtfs.zip">Download GTFS (local)</a>'
                du -h "site/${SLUG}/gtfs.zip" || true
              else
                any_fail=1
                if [ -f "site/${SLUG}/error.html" ]; then
                  LOCAL_NOTE='<em>Local GTFS not available — see <a href="error.html">error detail</a></em>'
                else
                  LOCAL_NOTE='<em>Local GTFS not available — see headers.txt/download.raw</em>'
                fi
              fi
              echo "::endgroup::"
            else
              any_fail=1
              URL_DISPLAY="(missing URL)"
              LOCAL_NOTE="<em>No source URL provided</em>"
            fi

            # --------------------------------------------
            # Compile with overrides (per-slug preferred)
            # --------------------------------------------
            OVR_FILE=""
            if [ -f "automation/overrides-${SLUG}.json" ]; then
              OVR_FILE="automation/overrides-${SLUG}.json"
            elif [ -f "automation/overrides.json" ]; then
              OVR_FILE="automation/overrides.json"
            fi
            echo "OVERRIDES for ${SLUG}: ${OVR_FILE:-none}"
            [ -n "${OVR_FILE}" ] && head -n 20 "${OVR_FILE}" || true

            # Run the builder; it downloads FEED_URL itself and applies OVERRIDES if provided
            if [ -n "${OVR_FILE}" ]; then
              FEED_SLUG="${SLUG}" \
              FEED_URL="${URL:-}" \
              FEED_API_HEADER="${API_HEADER:-}" \
              FEED_API_KEY="${API_KEY:-}" \
              OUT_DIR="site/${SLUG}" \
              OUT_ZIP="compiled.zip" \
              OUT_REPORT="report.json" \
              OVERRIDES="${OVR_FILE}" \
              node automation/rebuild-gtfs.js || any_fail=1
            else
              FEED_SLUG="${SLUG}" \
              FEED_URL="${URL:-}" \
              FEED_API_HEADER="${API_HEADER:-}" \
              FEED_API_KEY="${API_KEY:-}" \
              OUT_DIR="site/${SLUG}" \
              OUT_ZIP="compiled.zip" \
              OUT_REPORT="report.json" \
              node automation/rebuild-gtfs.js || any_fail=1
            fi

            # Per-feed index (no heredoc)
            INDEX_HTML='<!doctype html><meta charset="utf-8">
            <title>{{TITLE}}</title>
            <h1>{{TITLE}}</h1>
            <ul>
              <li><a href="compiled.zip">compiled.zip</a></li>
              <li><a href="'"${URL_DISPLAY//\"/&quot;}"'">Original source URL</a></li>
              <li><a href="headers.txt">Response headers</a> (debug)</li>
              <li><a href="report.json">report.json</a></li>
            </ul>
            <p>Last updated: <span id="ts"></span></p>
            <script>
              document.getElementById("ts").textContent = new Date().toISOString();
              (async()=>{
                try{
                  const r = await fetch("report.json",{cache:"no-store"});
                  if(!r.ok) return;
                  const rep = await r.json();
                  const p = document.createElement("p");
                  p.innerHTML = "Overrides total: <b>"+(rep.overrides?.total??0)+"</b> · source: <code>"+(rep.overridesSource||"")+"</code>";
                  document.body.appendChild(p);
                }catch{}
              })();
            </script>'
            printf "%s\n" "$INDEX_HTML" > "site/${SLUG}/index.html"
            sed -i "s/{{TITLE}}/${SLUG//\//-} GTFS (compiled)/g" "site/${SLUG}/index.html"

          done < feeds.tsv

          echo "Built tree under site/:"
          find site -maxdepth 2 -type f -printf "%p (%k KB)\n" | sort

          if [ "$any_fail" -ne 0 ]; then
            echo "One or more feeds failed to download or compile. See logs."
            exit 1
          fi

      - name: Generate registry.json (what’s built)
        shell: bash
        run: |
          set -euo pipefail
          node -e '
            const fs=require("fs");
            let slugs=[];
            try {
              slugs = fs.readFileSync("feeds.txt","utf8").split(/\r?\n/).filter(Boolean);
            } catch {}
            const registry = { generatedAt: new Date().toISOString(), slugsBuilt: slugs };
            fs.mkdirSync("site",{recursive:true});
            fs.writeFileSync("site/registry.json", JSON.stringify(registry,null,2));
          '

      - name: Root index (no heredoc)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p site
          INDEX_ROOT='<!doctype html><meta charset="utf-8">
          <title>GTFS Mapper Feeds</title>
          <h1>GTFS Mapper Feeds</h1>
          <p>Last updated: <span id="ts"></span></p>
          <ul id="list"></ul>
          <script>
            document.getElementById("ts").textContent = new Date().toISOString();
            (async () => {
              const res = await fetch("./feeds.json").catch(() => null);
              const data = res && res.ok ? await res.json() : [];
              const arr = Array.isArray(data) ? data : (data.feeds || []);
              const slugs = arr.map(x => typeof x === "string" ? x : x.slug).filter(Boolean);
              const ul = document.getElementById("list");
              for (const slug of slugs) {
                const li = document.createElement("li");
                const a = document.createElement("a");
                a.href = "./" + slug + "/";
                a.textContent = slug;
                li.appendChild(a);
                ul.appendChild(li);
              }
            })();
          </script>'
          printf "%s\n" "$INDEX_ROOT" > site/index.html

      - name: Expose feeds.json
        run: cp automation/feeds.json site/feeds.json

      - name: Add Admin dashboard (no heredoc)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p site/admin
          ADMIN_HTML='<!doctype html><meta charset="utf-8">
          <title>GTFS Admin</title>
          <style>
            body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;max-width:900px;margin:40px auto;padding:0 16px}
            h1{margin:0 0 8px}.muted{color:#666;margin:0 0 24px}
            table{border-collapse:collapse;width:100%;margin:16px 0}
            th,td{border:1px solid #ddd;padding:8px;text-align:left}
            th{background:#f7f7f7}
            .pill{display:inline-block;padding:2px 8px;border-radius:999px;font-size:12px}
            .ok{background:#e6ffec;color:#057a3b;border:1px solid #b8f5c7}
            .miss{background:#fff7e6;color:#8a5d00;border:1px solid #ffe0a3}
            .actions{display:flex;gap:8px}
            textarea{width:100%;min-height:140px;font-family:ui-monospace,Menlo,Consolas,monospace}
            .row{margin:20px 0}
          </style>
          <h1>GTFS Admin</h1>
          <p class="muted">Shows feeds from <code>feeds.json</code> and which ones are currently built on the site. Creates an <code>overrides.json</code> patch for the selected feed.</p>
          <div id="app">Loading…</div>
          <script>
          (async function(){
            const app = document.getElementById("app");
            const get = async (p, fallback)=>{ try{ const r=await fetch(p); if(!r.ok) return fallback; return await r.json(); }catch{return fallback;} };
            const feedsRaw = await get("../feeds.json", []);
            const feedsArr = Array.isArray(feedsRaw) ? feedsRaw : (feedsRaw.feeds || []);
            const feeds = feedsArr.map(x=>typeof x==="string"?{slug:x,url:null}:x).filter(x=>x&&x.slug);
            const reg = await get("../registry.json", { slugsBuilt: [] });
            const built = new Set(reg.slugsBuilt || []);
            const rows = feeds.map(f=>({ slug:f.slug, url:f.url||"", inSystem: built.has(f.slug) }));
            const mk=(t,a={},h="")=>{const e=document.createElement(t); for(const k in a)e.setAttribute(k,a[k]); e.innerHTML=h; return e;};
            const table=mk("table"); table.innerHTML="<thead><tr><th>Slug</th><th>URL</th><th>Status</th><th>Action</th></tr></thead>";
            const tbody=mk("tbody"); table.appendChild(tbody);
            for(const r of rows){
              const tr=mk("tr");
              tr.appendChild(mk("td",{},`<code>${r.slug}</code>`));
              tr.appendChild(mk("td",{}, r.url?`<a href=\"${r.url}\" target=\"_blank\" rel=\"noopener\">${r.url}</a>`:"<em class=\"muted\">—</em>"));
              tr.appendChild(mk("td",{}, r.inSystem?`<span class=\"pill ok\">in system</span>`:`<span class=\"pill miss\">not built</span>`));
              const tdAct=mk("td"); const btn=mk("button",{},"Create overrides patch"); btn.onclick=()=>openPatch(r.slug,r.url); tdAct.appendChild(btn); tr.appendChild(tdAct); tbody.appendChild(tr);
            }
            const panel=mk("div",{class:"row"});
            panel.innerHTML=`<h2>Create overrides.json patch</h2>
            <p class="muted">Pick a slug from the table above, then edit the JSON below and save it. Commit this file as <code>gtfs-mapper/automation/overrides.json</code>.</p>
            <div class="actions"><button id="download">Download overrides.json</button><button id="copy">Copy to clipboard</button></div>
            <div class="row"><textarea id="txt"></textarea></div>`;
            app.innerHTML=""; app.appendChild(table); app.appendChild(panel);
            const txt=panel.querySelector("#txt"); const btnDl=panel.querySelector("#download"); const btnCp=panel.querySelector("#copy");
            const existing = await get("../overrides.json", null); if(existing) txt.value = JSON.stringify(existing,null,2);
            function openPatch(slug,url){ const base=existing||{overrides:{}}; base.overrides[slug]=Object.assign({},base.overrides[slug]||{},{sourceUrl: url || (base.overrides[slug]?.sourceUrl || "")}); txt.value=JSON.stringify(base,null,2); txt.focus(); }
            btnDl.onclick=()=>{ const blob=new Blob([txt.value||"{}"],{type:"application/json"}); const a=document.createElement("a"); a.href=URL.createObjectURL(blob); a.download="overrides.json"; a.click(); URL.revokeObjectURL(a.href); };
            btnCp.onclick=async()=>{ try{ await navigator.clipboard.writeText(txt.value||""); btnCp.textContent="Copied!"; setTimeout(()=>btnCp.textContent="Copy to clipboard",1500);}catch{ alert("Copy failed; copy manually."); } };
          })();
          </script>'
          printf "%s\n" "$ADMIN_HTML" > site/admin/index.html

      - name: Publish overrides (if present)
        shell: bash
        run: |
          [ -f automation/overrides.json ] && cp automation/overrides.json site/overrides.json || true

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          # path is relative to repo root; working-directory is gtfs-mapper
          path: gtfs-mapper/site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4